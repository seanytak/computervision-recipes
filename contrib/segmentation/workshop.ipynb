{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6964035d",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Workshop\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Environment Setup\n",
    "2. Overview of Semantic Segmentation\n",
    "3. Labeling\n",
    "4. Data\n",
    "5. Modeling\n",
    "6. Running on AML\n",
    "7. Configuration Options\n",
    "8. MLFlow Parameter, Metrics, and Artifact Tracking\n",
    "9. Cleanup\n",
    "10. Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814da440",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "The following environments have been tested for this workshop\n",
    "\n",
    "- Linux (Ubuntu 18.04)\n",
    "- Azure Cloud Shell\n",
    "\n",
    "NOTE: This will replace the Azure ML CLI v1 extension with the public preview v2.\n",
    "\n",
    "### Linux Setup\n",
    "\n",
    "Run the `setup.sh` script via\n",
    "\n",
    "```bash\n",
    "bash setup.sh\n",
    "```\n",
    "\n",
    "If you run into permissions issues, you may have to run as root user via\n",
    "\n",
    "```bash\n",
    "sudo bash setup.sh\n",
    "```\n",
    "\n",
    "### Azure Cloud Shell\n",
    "\n",
    "If you cannot run the bash script on your environment, you can use the Azure Cloud Shell available in the Azure Portal.\n",
    "\n",
    "![Azure Cloud Shell](assets/AzureCloudShell.png)\n",
    "\n",
    "Copy and paste the content of `setup.sh` onto the terminal and your environment should be setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a365718",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "\n",
    "### Types of Segmentation\n",
    "\n",
    "![Semantic vs. Instance Segmentation](https://i.stack.imgur.com/MEB9F.png)\n",
    "\n",
    "For semantic segmentation, we are only interested in labeling a pixel as one of the $C$ available classes. For instance segmentation, an additional consideration on objects of the same class being distinct entities is also considered.\n",
    "\n",
    "The highlighted labeled region is referred to as the \"segmentation mask\".\n",
    "\n",
    "### Label Formats\n",
    "\n",
    "The following are some examples of different label formats you may encounter for the segmentation domain.\n",
    "\n",
    "1. Full Image Array (as an Image file .png, .jpg or in a tabular format .csv, .npy)\n",
    "   1. RGB / Class Label Based\n",
    "2. Compressed Format (such as Run-Length Encoding (RLE))\n",
    "3. Polygon Vertices (list of $x$, $y$ coordinates $[x_1, y_1, x_2, y_2, ..., x_n, y_n]$)\n",
    "\n",
    "### Labels for Modeling\n",
    "\n",
    "The standard form we would like to transform our data into is the following:\n",
    "\n",
    "- Input: Images with PyTorch shape convention of $(Ch, H, W) = \\text{(Channels, Height, Width)}$\n",
    "- Ground Truth: Masks with shape $ (H, W) = \\text{(Height, Width)} $.\n",
    "  - Each entry should be in the range $[0, C]$ where $ C = \\text{Number of Classes}$\n",
    "  - The entry $0$ is reserved for the \"background\" class (the absence of a class we are interested in)\n",
    "- Model Output: Predicted masks with shape $(C + 1, H, W) = \\text{(Classes, Height, Width)} $\n",
    "  - The $+1$ in $C + 1$ is due to the background class\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1c742",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will be using data from http://dronedataset.icg.tugraz.at to demonstrate the semantic segmentation codebase.\n",
    "\n",
    "The masks are provided as RGB images. To utilize the masks as the ground truth, we will need to convert each pixel into a class label instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from src.datasets.mask_labels import MaskLabelsDataset\n",
    "\n",
    "dataset = MaskLabelsDataset(\n",
    "    join(\"workshop_data\", \"train_labels/labels.csv\"), \n",
    "    mask_format=\"rgb\", \n",
    "    class_dict_path=join(\"workshop_data\", \"class_dict.csv\")\n",
    ")\n",
    "image, mask = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed29fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Height, Width, Ch)\n",
    "# This will be later rearranged to (Ch, Height, Width) per PyTorch convention\n",
    "image.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b95c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_segmentation_item(image: np.ndarray, mask: np.ndarray):\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "    axs[0][0].imshow(image)\n",
    "    axs[0][1].imshow(mask)\n",
    "    axs[1][0].imshow(image)\n",
    "    axs[1][0].imshow(mask, alpha=0.35)\n",
    "    \n",
    "display_segmentation_item(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0204f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.augmentation import preprocessing, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Deterministic Transformations onto our image for modeling\n",
    "# Usable as a one and done step via batch processing\n",
    "results = preprocessing(image=image, mask=mask)\n",
    "preprocessed_image, preprocessed_mask = results[\"image\"], results[\"mask\"]\n",
    "display_segmentation_item(preprocessed_image, preprocessed_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "# Stochastic and recomputed again for each batch\n",
    "# We are using albumentations for this\n",
    "results = augmentation(image=preprocessed_image, mask=preprocessed_mask)\n",
    "augmented_image, augmented_mask = results[\"image\"], results[\"mask\"]\n",
    "display_segmentation_item(augmented_image, augmented_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43af744a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Common metrics in the segmentation space are\n",
    "\n",
    "- Intersection over Union (IoU)\n",
    "  - Intuition: How much does my prediction overlap with the ground truth?\n",
    "- Average Precision and Mean Average Precision (AP, mAP)\n",
    "- Standard Classification Metrics\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-Score\n",
    "  \n",
    "### IoU\n",
    "\n",
    "![IoU](https://pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png)\n",
    "\n",
    "Properties to think about:\n",
    "- Either the ground truth or prediction could encapsulate the other, even with low IoU scores\n",
    "\n",
    "### Average Precision\n",
    "\n",
    "![Average Precision](https://kharshit.github.io/img/map_bboxes.png)\n",
    "\n",
    "![Average Precision](https://kharshit.github.io/img/map_gt.png)\n",
    "\n",
    "Typically you may see a metric like $AP@0.5$ which means Average Precision with IoU of 0.5.\n",
    "\n",
    "Average Precision matches the idea in the image classification space where we have a threshold in a multi-class classification scenario to determine true positives.\n",
    "\n",
    "### Standard Classification Metrics\n",
    "\n",
    "As segmentation based classifiers are classifying classes on a pixel level, the metrics for precision, recall, f1-score\n",
    "\n",
    "- Precision - What percentage of pixels for each class that were predicted are actually correct (true positive)\n",
    "- Recall - What percentage of pixels for each class were marked correctly (true positive)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60d22165",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Differing from the more familiar Image Classification scenario, in the object detection and segmentation domain our predictions are in the shape of an image.\n",
    "\n",
    "As a result, it's a natural result to use a convolutional layer as our final prediction layer. As a result, several architectures use fully convolutional networks.\n",
    "\n",
    "We leverage two popular models available from `torchvision`\n",
    "- FCN-ResNet50\n",
    "- DeepLabV3\n",
    "\n",
    "\n",
    "\n",
    "### DeepLabV3 Architecture\n",
    "\n",
    "![DeepLabV3 Architecture](https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-21_at_10.34.37_AM_qcoqzIU.png)\n",
    "\n",
    "#### Atrous Convolution\n",
    "\n",
    "![Atrous Convolution](https://miro.medium.com/max/395/1*SVkgHoFoiMZkjy54zM_SUw.gif)\n",
    "\n",
    "Integrated as just part of good ol' Conv2D in PyTorch parametrized by `dilation`. With `dilation=1` there is no additional space and is just the convolution you would expect.\n",
    "\n",
    "[All kinds of Convolution Animations](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08532708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.models.fcn_resnet50 import FCNResNet50\n",
    "\n",
    "model = FCNResNet50(24, pretrained=True, is_feature_extracting=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.deeplabv3 import DeepLabV3\n",
    "\n",
    "model = DeepLabV3(24, pretrained=True, is_feature_extracting=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1639a7",
   "metadata": {},
   "source": [
    "## Running on AML\n",
    "\n",
    "To run the training pipeline we utilize the Azure ML CLI to execute `preprocess.py` and `train.py` via `train_pipeline.yml`.\n",
    "\n",
    "```bash\n",
    "az ml job create --file train_pipeline.yml\n",
    "```\n",
    "\n",
    "The `preprocess.py` is specific to the Semantic Drone Dataset we are using for this workshop\n",
    "\n",
    "The `train.py` is a general training script that assumes a supported labels file format will be fed into it "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10953bbe",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Once the AML Training Pipeline has been properly connected to a data source of interest, there are two files that may require configuration\n",
    "\n",
    "1. Augmentation Configuration\n",
    "   1. Located at `config/augmentation.py`\n",
    "2. Experimentation Parameters (Model, Loss, Hyperparameters)\n",
    "   1. Located at `train_pipeline.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181afbf",
   "metadata": {},
   "source": [
    "## MLFlow Parameter Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c16296",
   "metadata": {},
   "source": [
    "## Experimentation Time\n",
    "\n",
    "As a lightweight exercise, let's look to try running some other experiments.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Modify some of the modeling parameters in `train_pipeline.yml`\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Modify the augmentation pipeline. Add an augmentation that you empirically believe to be valuable that is missing here.\n",
    "\n",
    "[Albumentations Augmentations Docs](https://albumentations.ai/docs/api_reference/augmentations/transforms/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449ecb7",
   "metadata": {},
   "source": [
    "## Thank you everyone for participating in the dry run!\n",
    "\n",
    "### Any Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48435e4",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "To delete all the resources created today, we can just delete the resource group.\n",
    "\n",
    "You can do so by running the cleanup script via\n",
    "\n",
    "```bash\n",
    "bash cleanup.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c29b479bbeade3e1d37076ed1c56a9811b51f5ffc00b849447c74e2da5ad5a06"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
